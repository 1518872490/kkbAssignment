# Q1: 如何使用用户标签来指导业务（如何提升业务）?

## A1:根据用户生命周期的三个阶段的不同，来考虑如何使用用户标签：

+ 获客：如何进行拉新，通过更精准的营销获取客户；
+ 粘客：个性化推荐，搜索排序，场景运营等；如：每日访问频率能否提升？停留的时间能否更长？
+ 留客：流失率预测，分析关键节点降低流失率。

# Q2: 如果给你一堆用户数据，没有打标签。你该如何处理（如何打标签）？

## A2: 遵循八字原则（用户消费行为分析），从以下4个维度打标签：

+ 用户标签：包括性别、年龄、地域、收入、学历、职业等用户属性；

+ 消费标签：包括消费习惯、购买意向、是否对促销敏感等；

+ 行为标签：时间段、频次、时长、收藏、点击、喜欢、评分；
  	（User Behavior可以分成Explicit Behavior和Implicit Behavior）；

+ 内容分析：对用户平时浏览的内容进行分析，比如体育、游戏、八卦。

  

  标签是对高维事物的抽象（降维），典型的打标签方式有PGC:专家生产、UGC:普通生产；常用的降维方法可以通过聚类算法，如：K-Means，EM聚类，Mean-Shift，DBSCAN，层次聚类，PCA

# Q3: 准确率和精确率有何不同（评估指标）?

## A3:

+ 准确率(Accuracy): $\frac{(TP+TN)}{(TP+TN+FN+FP)}$，即：预测正确的结果占总样本的百分比。反映模型分类正确的能力，但如果样本不平衡，准确率就会失效。
+ 精确率(Precision)又称查准率： $\frac{TP}{(TP+FP)}$ 即：预测结果中，预测为1(正样本)的结果中，实际也为1(正样本)的概率；也就是所有被预测为正的样本中实际为正的样本的占比。
+ 二者的区别：
  + 精准率Precision代表对正样本结果中的预测准确程度；
  + 而准确率Accuracy则代表对整体的预测准确程度，既包括正样本，也包括负样本。

# Q4: 如果你使用大众点评，想要给某个餐厅打标签。这时系统可以自动提示一些标签，你会如何设计（标签推荐）?

## A4:有如下方法：

+ 方法1：向我推荐整个大众点评的热门标签；
+ 方法2：向我推荐这个餐厅上面的最热门的标签；
+ 方法3：向我推荐我自己经常使用的标签；
+ 将方法2和3进行加权融合，生成最终的标签推荐结果。

# Q5: 使用TPOT等AutoML工具，有怎样的好处和不足?

## A5:

TPOT可以解决：特征选择，特征预处理、特征构建、模型选择、参数优化等机器学习流程，但不包括数据清洗。

+ 好处是方便，省去很多代码，可以帮我们融合出一个分数比较高的综合模型，并输出该最高分数模型的py文件，能为我们优化模型提供不错的参考；
+ 缺点是对算力要求比较高。处理小规模数据很快，但如果是大规模的数据问题，处理速度将很慢很慢，这时候可以抽样小部分数据跑一下TPOT。

#  Q6: 我们今天使用了10种方式来解MNIST，这些方法有何不同？你还有其他方法来解决MNIST识别问题么（分类方法）?

## A6:

### 10种解法：
  1. Logistic Regression 逻辑回归：
    逻辑回归模型属于分类模型，本质是将线性回归模型通过sigmoid函数，进行了一个非线性转换，得到一个介于0~1之间的概率值。它预测属于各个分类的概率，然后根据哪个概率最大，判断属于哪个分类。
    如果特征多为线性相关，但目标值为离散值时，可以考虑用逻辑回归。
    逻辑回归不容易过拟合，泛化能力强。
    逻辑回归算法需要提前做无量纲化处理。
  2. CART, ID3 决策树
    决策树既可以做分类模型，也可以做回归模型。
    决策树是通过信息增益entropy or 基尼系数 gini找到最高效的决策顺序，它能够生成清晰的基于特征(feature)选择不同预测结果的树状结构，具有可视化、可解释性强的优点，缺点是容易过拟合，泛化能力弱。
    决策树只考虑特征之间的相对距离，因此数据不需要做无量纲化处理。
  3. 朴素贝叶斯
    朴素贝叶斯既可以做分类模型，也可以做回归模型。
    它的前提是假定了特征与特征之间相互独立，且符合贝叶斯公式。核心思路是根据条件概率计算待判断点的类别。
    它的优点是对缺失数据不敏感，缺点是如果特征之间实际存在关联，将会影响结果。
    朴素贝叶斯模型有：伯努利贝叶斯、高斯贝叶斯、多项式贝叶斯三种模型。
    朴素贝叶斯在文本分类、文本过滤等以单词为特征的应用场景中表现不错。
  4. SVM支持向量机
    支持向量机算法既可以做分类模型，也可以做回归模型。
    SVM的思想：一些线性不可分的问题可能是非线性可分的，也就是在高维空间中存在分离超平面（separating hyperplane）。
    相当于一个弱神经网络模型，适合小数据集。它有一定的抗过拟合的能力，但牺牲了运算速度；
  5. KNN  K近邻
    K近邻算法既可以做分类模型，也可以做回归模型。
    K近邻算法的原理是对于一个新样本，K近邻的目的是在已有数据中找与它相似的K个数据，或者说“离它最近”的K个数据，如果这K个数据大多数属于同一个类别 ，则该样本也属于这个类别。
    更适用于小数据场景。
    优点：简单,易于理解,易于实现。
    缺点：
	  懒惰算法，对测试样本分类时的计算量大，内存开销大；
	  必须指定K值，K值选择不当则分类精度不能保证；
	  使用KNN算法时，数据需要做无量纲化处理。
  6. Adaboost自适应提升法
    Adaboost既可以做分类模型，也可以做回归模型。
    Adaboost根据前一次分类结果调整数据的权重，在上一个弱分类器中分类错误的样本权重 会在下一个若学习器中增加，分类正确的样本的权重则相应减少，并且在每一轮迭代时会向模型加入一个新的若学习器。不断重复调整权重和训练弱学习器，直到误分类数低于预设值或迭代次数达到指定最大值，最终得到一个强学习器。简单来说，Adaboost的核心思想就是调整错误样本的权重，进而迭代升级。
    它有3个优点：精度高，且灵活可调；几乎可以不用担心过拟合问题；简化特征工程流程；
    默认使用决策树分类/回归器作为弱学习器。
  7. XGBoost
    Adaboost既可以做分类模型，也可以做回归模型。
    是梯度提升决策树GBDT算法的改良版，都是利用了Boosting算法中的残差思想。
    不同的是，它的损失函数除了本身的损失，还加上了正则化部分，从而可以防止过拟合，泛化能力更强。它的损失 函数是对误差部分做了二阶泰勒展开，相较于GBDT算法的损失函数只对误差部分做负梯度(一阶泰勒)展开，更准确。
  8. LDA线性判别式分析
    LDA基本想法是使不同类别的样本尽量远离，同时使相同类别的样本尽可能靠近。这一目标通过扩大不同类别样本的类中心距离，同时缩小每个类 的类内方差来实现。
    LDA的分类过程分为两步：首先，使用权重向量w将样本空间投影到直线上；然后，寻找直线上的一个点，把正负样本分开。
  9. TPOT
    TPOT是基于Python的AutoML工具。它可以解决：特征选择，特征预处理、模型选择、参数优化等机器学习流程，但不包括数据清洗。
  + 处理小规模数据非常快，大规模数据非常慢
  + 目前只能做有监督学习
  + 支持的分类器主要有贝叶斯、决策树、集成树、SVM、KNN、线性模型、xgboost
  + 支持的回归器主要有决策树、集成树、线性模型、xgboost
  + 数据预处理：二值化、聚类、降维、标准化、正则化等
  + 特征选择：基于树模型、基于方差、基于F-值的百分比
  + 可以通过export()方法把训练过程导出为形式为sklearn pipeline的.py文件
  10. keras
    keras是基于 Python 的深度学习库，是Python语言编写的高级神经网络API。非常适合处理大型数据集的分类、回归问题。

### 其他分类方法：

解决MNIST识别问题属于分类问题，除了上面10种解法，还可以使用随机森林算法、梯度提升决策树GBDT算法。

